---
title: "Types"
---

## DBConfig

The `DBConfig` class specifies the storage location for the index, with options for in-memory storage, databases, or file-based storage.

### Parameters

| Parameter | Type | Default | Description |
|-------------------|----------------------------------|--------------|--------------------------------------------------------------------|
| `location` | `string` | - | DB location (`redis`, `postgres`, `memory`, `s3`, `gcs`, `local`) |
| `table_name` | `string` | None | _(Optional)_ Table name (`postgres`-only) |
| `connection_string` | `string` | None | _(Optional)_ Connection string to access DB |
| `bucket` | `string` | None | _(Optional)_ Bucket name for cloud storage (`s3`, `gcs`) |
| `access_key` | `string` | None | _(Optional)_ Access key for cloud storage |
| `secret_key` | `string` | None | _(Optional)_ Secret key for cloud storage |
| `region` | `string` | None | _(Optional)_ Region for cloud storage |
| `endpoint` | `string` | None | _(Optional)_ Custom endpoint for S3-compatible storage |
| `path` | `string` | None | _(Optional)_ Path for local file storage |

The supported `location` options are:

- `"redis"`: Use for high-speed, in-memory storage (recommended for `index_location`)
- `"postgres"`: Use for reliable, SQL-based storage (recommended for `config_location`)
- `"memory"`: Use for temporary in-memory storage (for benchmarking and evaluation purposes)
- `"s3"`: Use for Amazon S3 or S3-compatible storage
- `"gcs"`: Use for Google Cloud Storage
- `"local"`: Use for local file system storage

### Example Usage

```python
from cyborgdb_core import DBConfig

# Redis configuration
index_location = DBConfig(
    location="redis",
    connection_string="redis://localhost:6379"
)

# PostgreSQL configuration
config_location = DBConfig(
    location="postgres",
    table_name="config_table",
    connection_string="host=localhost dbname=vectordb user=postgres"
)

# S3 configuration
s3_location = DBConfig(
    location="s3",
    bucket="my-vector-index",
    access_key="YOUR_ACCESS_KEY",
    secret_key="YOUR_SECRET_KEY",
    region="us-east-1"
)

# Memory configuration (for testing)
memory_location = DBConfig(location="memory")
```

---

## Embeddings

The LangChain integration supports multiple embedding model types:

### Supported Embedding Types

| Type | Description | Example |
|------|-------------|---------|
| `str` | Model name string for SentenceTransformers | `"sentence-transformers/all-MiniLM-L6-v2"` |
| `SentenceTransformer` | SentenceTransformer model instance | `SentenceTransformer("all-MiniLM-L6-v2")` |
| `Embeddings` | Any LangChain Embeddings implementation | `OpenAIEmbeddings()`, `HuggingFaceEmbeddings()` |

### Example Usage

```python
from sentence_transformers import SentenceTransformer
from langchain_openai import OpenAIEmbeddings
from cyborgdb_core.langchain import CyborgVectorStore

# Using model name string
store1 = CyborgVectorStore(
    index_name="docs",
    index_key=key,
    api_key="your-api-key",
    embedding="sentence-transformers/all-MiniLM-L6-v2",  # String model name
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory")
)

# Using SentenceTransformer instance
model = SentenceTransformer("all-mpnet-base-v2")
store2 = CyborgVectorStore(
    index_name="docs",
    index_key=key,
    api_key="your-api-key",
    embedding=model,  # SentenceTransformer instance
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory")
)

# Using LangChain Embeddings
openai_embeddings = OpenAIEmbeddings()
store3 = CyborgVectorStore(
    index_name="docs",
    index_key=key,
    api_key="your-api-key",
    embedding=openai_embeddings,  # LangChain Embeddings
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory")
)
```

---

## DistanceMetric

`DistanceMetric` is a string representing the distance metric used for the index. Options include:

- `"cosine"`: Cosine similarity (recommended for normalized embeddings)
- `"euclidean"`: Euclidean distance
- `"squared_euclidean"`: Squared Euclidean distance

### Metric Characteristics

| Metric | Range | Best Match | Use Case |
|--------|-------|------------|----------|
| `cosine` | [0, 2] | 0 | Text embeddings, normalized vectors |
| `euclidean` | [0, ∞) | 0 | Raw feature vectors |
| `squared_euclidean` | [0, ∞) | 0 | When avoiding sqrt computation |

---

## IndexType

The index type determines the algorithm used for approximate nearest neighbor search.

### Available Index Types

| Type | Description | Speed | Recall | Index Size |
|------|-------------|-------|--------|------------|
| `"ivfflat"` | Inverted file with flat storage | Fast | Highest | Biggest |
| `"ivf"` | Inverted file with compression | Fastest | Lowest | Smallest |
| `"ivfpq"` | Inverted file with product quantization | Fast | High | Medium |

**Note**: `cyborgdb-lite` only supports `"ivfflat"` index type.

### Example Usage

```python
# IVFFlat index (highest recall)
store = CyborgVectorStore(
    index_name="high_recall_index",
    index_key=key,
    api_key="your-api-key",
    embedding="all-MiniLM-L6-v2",
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory"),
    index_type="ivfflat",
    index_config_params={"n_lists": 1024}
)

# IVFPQ index (balanced performance)
store = CyborgVectorStore(
    index_name="balanced_index",
    index_key=key,
    api_key="your-api-key",
    embedding="all-MiniLM-L6-v2",
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory"),
    index_type="ivfpq",
    index_config_params={
        "n_lists": 1024,
        "pq_dim": 64,
        "pq_bits": 8
    }
)
```

---

## IndexConfigParams

Optional parameters for configuring the index, passed as a dictionary.

### Parameters by Index Type

#### IVFFlat & IVF

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `n_lists` | `int` | 1024 | Number of inverted lists (clusters) |

#### IVFPQ

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `n_lists` | `int` | 1024 | Number of inverted lists (clusters) |
| `pq_dim` | `int` | 8 | Dimensionality after product quantization |
| `pq_bits` | `int` | 8 | Bits per quantized dimension (1-16) |

### Tuning Guidelines

<Tip>
- **n_lists**: Use √n where n is the expected number of vectors. Common values: 256, 512, 1024, 2048
- **pq_dim**: Should divide the embedding dimension evenly. Lower values = more compression
- **pq_bits**: 8 bits provides good balance. Lower = more compression, higher = better accuracy
</Tip>

---

## Document

LangChain Document object used for storing text with metadata.

### Attributes

| Attribute | Type | Description |
|-----------|------|-------------|
| `page_content` | `str` | The text content of the document |
| `metadata` | `dict` | Optional metadata associated with the document |

### Example Usage

```python
from langchain_core.documents import Document

# Create a document
doc = Document(
    page_content="This is the content of my document",
    metadata={
        "source": "manual",
        "author": "John Doe",
        "timestamp": "2024-01-01"
    }
)

# Add to vector store
store.add_documents([doc])
```

---

## Filter Format

Metadata filters use a dictionary format for querying documents.

### Simple Filters

```python
# Exact match
filter = {"category": "technology"}

# Multiple conditions (AND)
filter = {
    "category": "technology",
    "year": 2024
}
```

### Advanced Filters

```python
# Range queries
filter = {
    "price": {"$gte": 100, "$lte": 500}
}

# IN queries
filter = {
    "tags": {"$in": ["python", "machine-learning"]}
}

# Nested fields
filter = {
    "metadata.author": "John Doe"
}
```

### Supported Operators

| Operator | Description | Example |
|----------|-------------|---------|
| `$eq` | Equal to | `{"age": {"$eq": 25}}` |
| `$ne` | Not equal to | `{"status": {"$ne": "archived"}}` |
| `$gt` | Greater than | `{"price": {"$gt": 100}}` |
| `$gte` | Greater than or equal | `{"score": {"$gte": 0.8}}` |
| `$lt` | Less than | `{"quantity": {"$lt": 10}}` |
| `$lte` | Less than or equal | `{"rating": {"$lte": 5}}` |
| `$in` | In array | `{"tags": {"$in": ["ai", "ml"]}}` |
| `$nin` | Not in array | `{"category": {"$nin": ["draft", "deleted"]}}` |

---

## Return Types

### Query Results

Query operations return documents with optional scores:

```python
# similarity_search returns List[Document]
docs = store.similarity_search("query", k=5)
# Returns: [Document(...), Document(...), ...]

# similarity_search_with_score returns List[Tuple[Document, float]]
results = store.similarity_search_with_score("query", k=5)
# Returns: [(Document(...), 0.95), (Document(...), 0.87), ...]
```

### Score Normalization

Scores are normalized to [0, 1] range where:
- **1.0** = Perfect match
- **0.0** = Worst match

The normalization depends on the distance metric used.

---

## Async Support

All methods have async variants prefixed with `a`:

| Sync Method | Async Method |
|-------------|--------------|
| `add_texts` | `aadd_texts` |
| `add_documents` | `aadd_documents` |
| `similarity_search` | `asimilarity_search` |
| `similarity_search_with_score` | `asimilarity_search_with_score` |
| `max_marginal_relevance_search` | `amax_marginal_relevance_search` |
| `delete` | `adelete` |

### Example Usage

```python
import asyncio

async def main():
    # Async text addition
    ids = await store.aadd_texts(["async text 1", "async text 2"])
    
    # Async search
    docs = await store.asimilarity_search("query", k=5)
    
    # Async deletion
    success = await store.adelete(ids)

asyncio.run(main())
```

# __init__.mdx

---
title: "__init__"
mode: "wide"
---

Initializes a new CyborgVectorStore instance for use with LangChain.

```python
CyborgVectorStore(
    index_name: str,
    index_key: bytes,
    api_key: str,
    embedding: Union[str, Embeddings, SentenceTransformer],
    index_location: DBConfig,
    config_location: DBConfig,
    items_location: Optional[DBConfig] = None,
    index_type: str = "ivfflat",
    index_config_params: Optional[Dict[str, Any]] = None,
    dimension: Optional[int] = None,
    metric: str = "cosine"
)
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `index_name` | `str` | Name of the index (must be unique) |
| `index_key` | `bytes` | 32-byte encryption key for securing index data |
| `api_key` | `str` | API key for CyborgDB authentication |
| `embedding` | `Union[str, Embeddings, SentenceTransformer]` | Embedding model name or instance |
| `index_location` | `DBConfig` | Configuration for index data storage location |
| `config_location` | `DBConfig` | Configuration for index config storage location |
| `items_location` | `Optional[DBConfig]` | _(Optional)_ Location for item data storage (default: memory) |
| `index_type` | `str` | Type of index: "ivfflat", "ivf", or "ivfpq" (default: "ivfflat") |
| `index_config_params` | `Optional[Dict[str, Any]]` | _(Optional)_ Additional index configuration parameters |
| `dimension` | `Optional[int]` | _(Optional)_ Embedding dimension (auto-inferred if not provided) |
| `metric` | `str` | Distance metric: "cosine", "euclidean", or "squared_euclidean" (default: "cosine") |

### Returns

`CyborgVectorStore`: Initialized vector store instance

### Exceptions

<AccordionGroup>
    <Accordion title="ValueError">
        - Throws if index_type is invalid
        - Throws if using cyborgdb-lite with non-ivfflat index type
    </Accordion>
    <Accordion title="RuntimeError">
        - Throws if no embedding model provided and dimension not specified
    </Accordion>
</AccordionGroup>

### Example Usage

```python
from cyborgdb_core.langchain import CyborgVectorStore
from cyborgdb_core import DBConfig

# Generate a secure key
index_key = CyborgVectorStore.generate_key()

# Initialize with string embedding model name
store = CyborgVectorStore(
    index_name="my_documents",
    index_key=index_key,
    api_key="your-api-key",
    embedding="sentence-transformers/all-MiniLM-L6-v2",
    index_location=DBConfig("s3", bucket="my-bucket"),
    config_location=DBConfig("s3", bucket="my-bucket"),
    index_type="ivfflat",
    metric="cosine"
)
```

---

# get_embeddings.mdx

---
title: "get_embeddings"
mode: "wide"
---

Generates embeddings for the given texts using the configured embedding model.

```python
get_embeddings(texts: Union[str, List[str]]) -> np.ndarray
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `texts` | `Union[str, List[str]]` | Single text string or list of text strings to embed |

### Returns

`np.ndarray`: NumPy array of embeddings
- If single text: 1-D array of shape `(dimension,)`
- If list of texts: 2-D array of shape `(num_texts, dimension)`

### Exceptions

<AccordionGroup>
    <Accordion title="RuntimeError">
        - Throws if no embedding model is available
    </Accordion>
    <Accordion title="TypeError">
        - Throws if embedding model type is not supported
    </Accordion>
</AccordionGroup>

### Example Usage

```python
# Single text embedding
embedding = store.get_embeddings("Hello, world!")
print(f"Embedding shape: {embedding.shape}")  # (384,)

# Multiple text embeddings
texts = ["First document", "Second document", "Third document"]
embeddings = store.get_embeddings(texts)
print(f"Embeddings shape: {embeddings.shape}")  # (3, 384)
```

---

# add_texts.mdx

---
title: "add_texts"
mode: "wide"
---

Adds text documents to the vector store with optional metadata.

```python
add_texts(
    texts: Iterable[str],
    metadatas: Optional[List[dict]] = None,
    ids: Optional[List[str]] = None,
    **kwargs
) -> List[str]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `texts` | `Iterable[str]` | Iterable of text strings to add |
| `metadatas` | `Optional[List[dict]]` | _(Optional)_ List of metadata dictionaries for each text |
| `ids` | `Optional[List[str]]` | _(Optional)_ List of IDs for the texts (auto-generated if not provided) |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[str]`: List of IDs for the added texts

### Exceptions

<AccordionGroup>
    <Accordion title="ValueError">
        - Throws if length of ids doesn't match length of texts
    </Accordion>
</AccordionGroup>

### Example Usage

```python
# Add texts with auto-generated IDs
texts = ["Document about AI", "Document about ML", "Document about NLP"]
ids = store.add_texts(texts)
print(f"Added {len(ids)} documents")

# Add texts with metadata and custom IDs
texts = ["Python tutorial", "JavaScript guide"]
metadatas = [
    {"language": "python", "level": "beginner"},
    {"language": "javascript", "level": "intermediate"}
]
custom_ids = ["doc1", "doc2"]
ids = store.add_texts(texts, metadatas=metadatas, ids=custom_ids)
```

---

# add_documents.mdx

---
title: "add_documents"
mode: "wide"
---

Adds LangChain Document objects to the vector store.

```python
add_documents(
    documents: List[Document],
    ids: Optional[List[str]] = None,
    **kwargs
) -> List[str]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `documents` | `List[Document]` | List of LangChain Document objects to add |
| `ids` | `Optional[List[str]]` | _(Optional)_ List of IDs for the documents (auto-generated if not provided) |
| `**kwargs` | `Any` | Additional keyword arguments passed to add_texts |

### Returns

`List[str]`: List of IDs for the added documents

### Example Usage

```python
from langchain_core.documents import Document

# Create documents with metadata
documents = [
    Document(
        page_content="Introduction to machine learning",
        metadata={"chapter": 1, "topic": "ML basics"}
    ),
    Document(
        page_content="Deep learning fundamentals",
        metadata={"chapter": 2, "topic": "Neural networks"}
    )
]

# Add documents to the store
ids = store.add_documents(documents)
print(f"Added {len(ids)} documents")
```

---

# delete.mdx

---
title: "delete"
mode: "wide"
---

Deletes documents from the vector store or deletes the entire index.

```python
delete(
    ids: Optional[List[str]] = None,
    delete_index: bool = False
) -> bool
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `ids` | `Optional[List[str]]` | _(Optional)_ List of document IDs to delete |
| `delete_index` | `bool` | If True, deletes the entire index regardless of `ids` (default: False) |

### Returns

`bool`: True if deletion was successful, False otherwise

### Example Usage

```python
# Delete specific documents
doc_ids = ["doc1", "doc2", "doc3"]
success = store.delete(ids=doc_ids)
if success:
    print("Documents deleted successfully")

# Delete the entire index
success = store.delete(delete_index=True)
if success:
    print("Index deleted successfully")
```

---

# similarity_search.mdx

---
title: "similarity_search"
mode: "wide"
---

Returns documents most similar to the query text.

```python
similarity_search(
    query: str,
    k: int = 4,
    filter: Optional[Dict[str, Any]] = None,
    **kwargs
) -> List[Document]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `query` | `str` | Query text to search for |
| `k` | `int` | Number of documents to return (default: 4) |
| `filter` | `Optional[Dict[str, Any]]` | _(Optional)_ Metadata filters to apply |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[Document]`: List of most similar Document objects

### Example Usage

```python
# Basic similarity search
results = store.similarity_search("machine learning algorithms", k=5)
for doc in results:
    print(f"Content: {doc.page_content[:100]}...")
    print(f"Metadata: {doc.metadata}")

# Search with metadata filter
filter_dict = {"language": "python", "level": "beginner"}
results = store.similarity_search(
    "python tutorial",
    k=3,
    filter=filter_dict
)
```

---

# similarity_search_by_vector.mdx

---
title: "similarity_search_by_vector"
mode: "wide"
---

Returns documents most similar to an embedding vector.

```python
similarity_search_by_vector(
    embedding: Union[List[float], np.ndarray],
    k: int = 4,
    filter: Optional[Dict[str, Any]] = None,
    **kwargs
) -> List[Document]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `embedding` | `Union[List[float], np.ndarray]` | Embedding vector to search with |
| `k` | `int` | Number of documents to return (default: 4) |
| `filter` | `Optional[Dict[str, Any]]` | _(Optional)_ Metadata filters to apply |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[Document]`: List of most similar Document objects

### Example Usage

```python
# Get embedding for a query
query_embedding = store.get_embeddings("data science concepts")

# Search using the embedding
results = store.similarity_search_by_vector(query_embedding, k=5)

# Search with custom embedding
custom_embedding = np.random.rand(384)  # Example 384-dim embedding
results = store.similarity_search_by_vector(custom_embedding, k=3)
```

---

# similarity_search_with_score.mdx

---
title: "similarity_search_with_score"
mode: "wide"
---

Returns documents most similar to the query along with relevance scores.

```python
similarity_search_with_score(
    query: str,
    k: int = 4,
    filter: Optional[Dict[str, Any]] = None,
    **kwargs
) -> List[Tuple[Document, float]]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `query` | `str` | Query text to search for |
| `k` | `int` | Number of documents to return (default: 4) |
| `filter` | `Optional[Dict[str, Any]]` | _(Optional)_ Metadata filters to apply |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[Tuple[Document, float]]`: List of (Document, score) tuples where score is normalized [0, 1]

### Example Usage

```python
# Search with scores
results = store.similarity_search_with_score("neural networks", k=3)

for doc, score in results:
    print(f"Score: {score:.4f}")
    print(f"Content: {doc.page_content[:100]}...")
    print(f"Metadata: {doc.metadata}")
    print("---")

# Filter results by score threshold
threshold = 0.7
high_score_results = [
    (doc, score) for doc, score in results if score >= threshold
]
```

---

# similarity_search_with_relevance_scores.mdx

---
title: "_similarity_search_with_relevance_scores"
mode: "wide"
---

Returns documents with relevance scores in the range [0, 1], with optional score filtering.

```python
_similarity_search_with_relevance_scores(
    query: str,
    k: int,
    **kwargs
) -> List[Tuple[Document, float]]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `query` | `str` | Query text to search for |
| `k` | `int` | Number of documents to return |
| `**kwargs` | `Any` | Additional arguments including `filter` and `score_threshold` |

### Keyword Arguments

| Parameter | Type | Description |
|-----------|------|-------------|
| `filter` | `Dict[str, Any]` | _(Optional)_ Metadata filters to apply |
| `score_threshold` | `float` | _(Optional)_ Minimum score threshold for results |

### Returns

`List[Tuple[Document, float]]`: List of (Document, score) tuples with scores in [0, 1]

### Example Usage

```python
# Search with relevance scores and threshold
results = store._similarity_search_with_relevance_scores(
    "machine learning",
    k=10,
    score_threshold=0.5
)

print(f"Found {len(results)} documents above threshold")
for doc, score in results:
    print(f"Relevance: {score:.2%} - {doc.page_content[:50]}...")
```

---

# max_marginal_relevance_search.mdx

---
title: "max_marginal_relevance_search"
mode: "wide"
---

Returns documents selected using maximal marginal relevance to balance relevance and diversity.

```python
max_marginal_relevance_search(
    query: str,
    k: int = 4,
    fetch_k: int = 20,
    lambda_mult: float = 0.5,
    filter: Optional[Dict[str, Any]] = None,
    **kwargs
) -> List[Document]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `query` | `str` | Query text to search for |
| `k` | `int` | Number of documents to return (default: 4) |
| `fetch_k` | `int` | Number of documents to fetch before reranking (default: 20) |
| `lambda_mult` | `float` | Diversity control: 0 = max diversity, 1 = min diversity (default: 0.5) |
| `filter` | `Optional[Dict[str, Any]]` | _(Optional)_ Metadata filters to apply |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[Document]`: List of diverse, relevant Document objects

### Example Usage

```python
# Search with balanced relevance and diversity
results = store.max_marginal_relevance_search(
    "Python programming concepts",
    k=5,
    fetch_k=20,
    lambda_mult=0.5  # Balance relevance and diversity
)

# Maximum diversity (avoid similar documents)
diverse_results = store.max_marginal_relevance_search(
    "data structures",
    k=5,
    lambda_mult=0.0  # Maximum diversity
)

# Maximum relevance (similar to standard search)
relevant_results = store.max_marginal_relevance_search(
    "algorithms",
    k=5,
    lambda_mult=1.0  # Maximum relevance
)
```

---

# max_marginal_relevance_search_by_vector.mdx

---
title: "max_marginal_relevance_search_by_vector"
mode: "wide"
---

Returns documents selected using maximal marginal relevance with an embedding vector.

```python
max_marginal_relevance_search_by_vector(
    embedding: Union[List[float], np.ndarray],
    k: int = 4,
    fetch_k: int = 20,
    lambda_mult: float = 0.5,
    filter: Optional[Dict[str, Any]] = None,
    **kwargs
) -> List[Document]
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `embedding` | `Union[List[float], np.ndarray]` | Query embedding vector |
| `k` | `int` | Number of documents to return (default: 4) |
| `fetch_k` | `int` | Number of documents to fetch before reranking (default: 20) |
| `lambda_mult` | `float` | Diversity control: 0 = max diversity, 1 = min diversity (default: 0.5) |
| `filter` | `Optional[Dict[str, Any]]` | _(Optional)_ Metadata filters to apply |
| `**kwargs` | `Any` | Additional keyword arguments (currently unused) |

### Returns

`List[Document]`: List of diverse, relevant Document objects

### Example Usage

```python
# Get query embedding
query_embedding = store.get_embeddings("machine learning algorithms")

# MMR search with embedding
results = store.max_marginal_relevance_search_by_vector(
    query_embedding,
    k=5,
    fetch_k=30,
    lambda_mult=0.3  # Favor diversity
)

# With metadata filter
filtered_results = store.max_marginal_relevance_search_by_vector(
    query_embedding,
    k=4,
    filter={"category": "tutorial"},
    lambda_mult=0.7  # Favor relevance
)
```

---

# from_texts.mdx

---
title: "from_texts"
mode: "wide"
---

Creates a CyborgVectorStore instance from a list of texts.

```python
@classmethod
from_texts(
    texts: List[str],
    embedding: Union[str, Embeddings, SentenceTransformer],
    metadatas: Optional[List[dict]] = None,
    **kwargs
) -> CyborgVectorStore
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `texts` | `List[str]` | List of text strings to add to the store |
| `embedding` | `Union[str, Embeddings, SentenceTransformer]` | Embedding model or model name |
| `metadatas` | `Optional[List[dict]]` | _(Optional)_ List of metadata dictionaries for each text |
| `**kwargs` | `Any` | Additional arguments passed to constructor and add_texts |

### Keyword Arguments

| Parameter | Type | Description |
|-----------|------|-------------|
| `ids` | `List[str]` | _(Optional)_ IDs for the texts |
| `index_name` | `str` | Name of the index (default: "langchain_index") |
| `index_key` | `bytes` | 32-byte encryption key (required) |
| `api_key` | `str` | API key for CyborgDB (required) |
| `index_location` | `DBConfig` | Index storage location (required) |
| `config_location` | `DBConfig` | Config storage location (required) |
| `index_type` | `str` | Index type (default: "ivfflat") |
| `metric` | `str` | Distance metric (default: "cosine") |

### Returns

`CyborgVectorStore`: Initialized vector store with texts added

### Exceptions

<AccordionGroup>
    <Accordion title="ValueError">
        - Throws if index_key is not provided
    </Accordion>
</AccordionGroup>

### Example Usage

```python
from cyborgdb_core import DBConfig

# Create store from texts
texts = [
    "Introduction to Python",
    "Advanced Python techniques",
    "Python for data science"
]
metadatas = [
    {"chapter": 1}, 
    {"chapter": 2}, 
    {"chapter": 3}
]

store = CyborgVectorStore.from_texts(
    texts=texts,
    embedding="sentence-transformers/all-MiniLM-L6-v2",
    metadatas=metadatas,
    index_name="python_docs",
    index_key=CyborgVectorStore.generate_key(),
    api_key="your-api-key",
    index_location=DBConfig("s3", bucket="my-bucket"),
    config_location=DBConfig("s3", bucket="my-bucket")
)
```

---

# from_documents.mdx

---
title: "from_documents"
mode: "wide"
---

Creates a CyborgVectorStore instance from a list of LangChain Document objects.

```python
@classmethod
from_documents(
    documents: List[Document],
    embedding: Union[str, Embeddings, SentenceTransformer],
    **kwargs
) -> CyborgVectorStore
```

### Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `documents` | `List[Document]` | List of LangChain Document objects |
| `embedding` | `Union[str, Embeddings, SentenceTransformer]` | Embedding model or model name |
| `**kwargs` | `Any` | Additional arguments passed to from_texts |

### Returns

`CyborgVectorStore`: Initialized vector store with documents added

### Example Usage

```python
from langchain_core.documents import Document
from cyborgdb_core import DBConfig

# Create documents
documents = [
    Document(
        page_content="Introduction to natural language processing",
        metadata={"source": "nlp_guide.pdf", "page": 1}
    ),
    Document(
        page_content="Tokenization and text preprocessing",
        metadata={"source": "nlp_guide.pdf", "page": 15}
    ),
    Document(
        page_content="Word embeddings and semantic similarity",
        metadata={"source": "nlp_guide.pdf", "page": 42}
    )
]

# Create store from documents
store = CyborgVectorStore.from_documents(
    documents=documents,
    embedding="sentence-transformers/all-mpnet-base-v2",
    index_name="nlp_documents",
    index_key=CyborgVectorStore.generate_key(),
    api_key="your-api-key",
    index_location=DBConfig("memory"),
    config_location=DBConfig("memory")
)
```

---

# generate_key.mdx

---
title: "generate_key"
mode: "wide"
---

Generates a secure random 32-byte encryption key for index encryption.

```python
@staticmethod
generate_key() -> bytes
```

### Returns

`bytes`: Cryptographically secure 32-byte key

### Example Usage

```python
# Generate a new encryption key
index_key = CyborgVectorStore.generate_key()
print(f"Key length: {len(index_key)} bytes")

# Use the key to create a store
store = CyborgVectorStore(
    index_name="secure_index",
    index_key=index_key,
    api_key="your-api-key",
    embedding="all-MiniLM-L6-v2",
    index_location=DBConfig("s3", bucket="secure-bucket"),
    config_location=DBConfig("s3", bucket="secure-bucket")
)

# Store the key securely for future use
import base64
encoded_key = base64.b64encode(index_key).decode('utf-8')
print(f"Base64 encoded key: {encoded_key}")
```

### Security Notes

<Warning>
Store encryption keys securely! Never commit keys to version control or expose them in logs. Consider using a key management service for production deployments.
</Warning>